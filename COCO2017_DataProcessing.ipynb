{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pycocotools.coco import COCO\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6538113",
   "metadata": {},
   "source": [
    "# 1. Filter Classes and Re-label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ca13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_IDs(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    categories = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "    print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3951b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_IDs(\"/mnt/datassd0/coco-2017/annotations/instances_train2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d19c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_IDs(\"/mnt/datassd0/coco-2017/annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# KEEP IMAGES WITH 0 ANNOTATIONS AND ASSIGN \"UNLABELED\" CATEGORY #################\n",
    "def filter_and_relabel_coco(input_json, output_json, dropped_classes):\n",
    "    \n",
    "    # Load the original COCO dataset\n",
    "    with open(input_json, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Get a mapping of original category IDs to names\n",
    "    original_categories = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "\n",
    "    # Select only categories **not in dropped_classes** and relabel them from 1 to N\n",
    "    selected_categories = [c for c in coco_data[\"categories\"] if c[\"name\"] not in dropped_classes]\n",
    "    selected_cat_ids = {c[\"id\"]: i + 1 for i, c in enumerate(selected_categories)}  # Start from 1 instead of 0\n",
    "\n",
    "    # Add an \"unlabeled\" category at the end\n",
    "    unlabeled_id = len(selected_categories) + 1\n",
    "    new_categories = [{\"id\": new_id, \"name\": c[\"name\"]} for new_id, c in enumerate(selected_categories, start=1)]\n",
    "    new_categories.append({\"id\": unlabeled_id, \"name\": \"unlabeled\"})  # Add \"unlabeled\" category\n",
    "\n",
    "    new_annotations = []\n",
    "    image_id_to_annotations = {}\n",
    "\n",
    "    # Process annotations and keep only non-dropped categories\n",
    "    for ann in coco_data[\"annotations\"]:\n",
    "        if ann[\"category_id\"] in selected_cat_ids:\n",
    "            ann[\"category_id\"] = selected_cat_ids[ann[\"category_id\"]]  # Re-map category ID\n",
    "            new_annotations.append(ann)\n",
    "            image_id_to_annotations.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "    # Get all image IDs that exist in the dataset\n",
    "    all_image_ids = {img[\"id\"] for img in coco_data[\"images\"]}\n",
    "\n",
    "    # Identify images that have no remaining annotations\n",
    "    images_without_annotations = all_image_ids - set(image_id_to_annotations.keys())\n",
    "\n",
    "    # Assign a dummy \"unlabeled\" annotation to images that lost all annotations\n",
    "    for img_id in images_without_annotations:\n",
    "        new_annotations.append({\n",
    "            \"id\": len(new_annotations) + 1,  # Unique annotation ID\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": unlabeled_id,  # Assign to \"unlabeled\"\n",
    "            \"bbox\": [0, 0, 1, 1],  # Placeholder bounding box\n",
    "            \"area\": 1,\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "\n",
    "    # Keep all images, even if they had 0 annotations\n",
    "    new_images = coco_data[\"images\"]\n",
    "\n",
    "    new_coco_data = {\n",
    "        \"info\": coco_data.get(\"info\", {}),\n",
    "        \"licenses\": coco_data.get(\"licenses\", []),\n",
    "        \"images\": new_images,\n",
    "        \"annotations\": new_annotations,\n",
    "        \"categories\": new_categories,\n",
    "    }\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(new_coco_data, f, indent=None)\n",
    "\n",
    "    print(f\"Filtered COCO dataset saved as {output_json}\")\n",
    "\n",
    "# Define dropped classes\n",
    "dropped_classes = [\n",
    "    \"person\", # Highest class label\n",
    "    \"wine glass\", \"cup\", \"bicycle\", \"potted plant\", \"bowl\", # Repetitive classes\n",
    "    \"snowboard\", \"surfboard\", \"baseball glove\", \"baseball bat\", # Sports category (highly associated with person)\n",
    "    \"tennis racket\", \"kite\", \"frisbee\", \"skis\", \"sports ball\", \"skateboard\", # Sports category (highly associated with person)\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", # Accessories category (highly associated with person)\n",
    "    \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\" # Vehicles category (highly associated with person)\n",
    "    \"dining table\", # Single class highly associated with person\n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", # Outdoor objects\n",
    "    \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", # Outdoor wild animals\n",
    "]\n",
    "\n",
    "\n",
    "# Apply the filtering function\n",
    "filter_and_relabel_coco(\"/mnt/datassd0/coco-2017/annotations/instances_train2017.json\", \"/mnt/datassd0/coco-2017/annotations/instances_train2017_filtered.json\", dropped_classes)\n",
    "filter_and_relabel_coco(\"/mnt/datassd0/coco-2017/annotations/instances_val2017.json\", \"/mnt/datassd0/coco-2017/annotations/instances_val2017_filtered.json\", dropped_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_IDs(\"/mnt/datassd0/coco-2017/annotations/instances_train2017_filtered.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d920fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_IDs(\"/mnt/datassd0/coco-2017/annotations/instances_val2017_filtered.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e681f88",
   "metadata": {},
   "source": [
    "# 2. Split Data into Train and Test Sets (80-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f89df0",
   "metadata": {},
   "source": [
    "### This Cell Splits annotations 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8dc8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_split_coco(train_json, val_json, output_train_json, output_val_json, train_ratio=0.8):\n",
    "    # Load train and validation COCO datasets\n",
    "    with open(train_json, \"r\") as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(val_json, \"r\") as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    # Merge images and annotations from both datasets\n",
    "    all_images = train_data[\"images\"] + val_data[\"images\"]\n",
    "    all_annotations = train_data[\"annotations\"] + val_data[\"annotations\"]\n",
    "    categories = train_data[\"categories\"]  # Assuming both datasets have the same category list\n",
    "\n",
    "    # Create a dictionary to store image annotations\n",
    "    image_id_to_annotations = {img[\"id\"]: [] for img in all_images}\n",
    "    for ann in all_annotations:\n",
    "        image_id_to_annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    # List all image IDs\n",
    "    all_image_ids = list(image_id_to_annotations.keys())\n",
    "\n",
    "    # Extract labels: Use the first class ID of each image or assign -1 if no annotations exist\n",
    "    image_labels = []\n",
    "    for img_id in all_image_ids:\n",
    "        labels = [ann[\"category_id\"] for ann in image_id_to_annotations[img_id]]\n",
    "        image_labels.append(labels[0] if labels else -1)  # Assign -1 to unlabeled images\n",
    "\n",
    "    # Perform an 80-20 stratified split\n",
    "    train_ids, val_ids = train_test_split(\n",
    "        all_image_ids, train_size=train_ratio, stratify=image_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert lists to sets for faster membership checking\n",
    "    train_ids_set = set(train_ids)\n",
    "    val_ids_set = set(val_ids)\n",
    "\n",
    "    # Create new train and validation datasets\n",
    "    new_train_images = [img for img in all_images if img[\"id\"] in train_ids_set]\n",
    "    new_val_images = [img for img in all_images if img[\"id\"] in val_ids_set]\n",
    "\n",
    "    new_train_annotations = [ann for ann in all_annotations if ann[\"image_id\"] in train_ids_set]\n",
    "    new_val_annotations = [ann for ann in all_annotations if ann[\"image_id\"] in val_ids_set]\n",
    "\n",
    "    # Save new train JSON\n",
    "    new_train_data = {\n",
    "        \"info\": train_data.get(\"info\", {}),\n",
    "        \"licenses\": train_data.get(\"licenses\", []),\n",
    "        \"images\": new_train_images,\n",
    "        \"annotations\": new_train_annotations,\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "    with open(output_train_json, \"w\") as f:\n",
    "        json.dump(new_train_data, f, indent=None)\n",
    "\n",
    "    # Save new validation JSON\n",
    "    new_val_data = {\n",
    "        \"info\": train_data.get(\"info\", {}),\n",
    "        \"licenses\": train_data.get(\"licenses\", []),\n",
    "        \"images\": new_val_images,\n",
    "        \"annotations\": new_val_annotations,\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "    with open(output_val_json, \"w\") as f:\n",
    "        json.dump(new_val_data, f, indent=None)\n",
    "\n",
    "    print(f\"New train dataset: {len(new_train_images)} images, {len(new_train_annotations)} annotations\")\n",
    "    print(f\"New validation dataset: {len(new_val_images)} images, {len(new_val_annotations)} annotations\")\n",
    "\n",
    "# Define file paths\n",
    "train_json_path = \"/mnt/datassd0/coco-2017/annotations/instances_train2017_filtered.json\"\n",
    "val_json_path = \"/mnt/datassd0/coco-2017/annotations/instances_val2017_filtered.json\"\n",
    "output_train_json = \"/mnt/datassd0/coco-2017/annotations/instances_train2017_balanced.json\"\n",
    "output_val_json = \"/mnt/datassd0/coco-2017/annotations/instances_val2017_balanced.json\"\n",
    "\n",
    "# Perform combination and split\n",
    "combine_and_split_coco(train_json_path, val_json_path, output_train_json, output_val_json, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ff2d0",
   "metadata": {},
   "source": [
    "### This Cell re-splits the original train/val images files from 95-5 split to 80-20 split to match new annotaitons\n",
    "#### This takes some time to copy all images into new folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original image directories\n",
    "# original_train_dir = \"train2017\"\n",
    "# original_val_dir = \"val2017\"\n",
    "\n",
    "# # New balanced image directories\n",
    "# new_train_dir = \"balanced_train2017\"\n",
    "# new_val_dir   = \"balanced_val2017\"\n",
    "\n",
    "# # Create new directories if they don't already exist\n",
    "# os.makedirs(new_train_dir, exist_ok=True)\n",
    "# os.makedirs(new_val_dir, exist_ok=True)\n",
    "\n",
    "# # Paths to the balanced JSON files\n",
    "# balanced_train_json = \"annotations/instances_train2017_balanced.json\"\n",
    "# balanced_val_json   = \"annotations/instances_val2017_balanced.json\"\n",
    "\n",
    "# # Load balanced train JSON\n",
    "# with open(balanced_train_json, \"r\") as f:\n",
    "#     train_data = json.load(f)\n",
    "\n",
    "# # Load balanced validation JSON\n",
    "# with open(balanced_val_json, \"r\") as f:\n",
    "#     val_data = json.load(f)\n",
    "\n",
    "# def copy_images(image_list, destination_dir):\n",
    "#     for img in image_list:\n",
    "#         file_name = img[\"file_name\"]\n",
    "#         src_path = None\n",
    "\n",
    "#         # Check if the image exists in the original training folder\n",
    "#         train_path = os.path.join(original_train_dir, file_name)\n",
    "#         if os.path.exists(train_path):\n",
    "#             src_path = train_path\n",
    "#         else:\n",
    "#             # Otherwise, check the original validation folder\n",
    "#             val_path = os.path.join(original_val_dir, file_name)\n",
    "#             if os.path.exists(val_path):\n",
    "#                 src_path = val_path\n",
    "\n",
    "#         if src_path is None:\n",
    "#             print(f\"Warning: Image {file_name} not found in either directory.\")\n",
    "#             continue\n",
    "\n",
    "#         dst_path = os.path.join(destination_dir, file_name)\n",
    "#         shutil.copy2(src_path, dst_path)\n",
    "\n",
    "# # Copy images for the new training set\n",
    "# copy_images(train_data[\"images\"], new_train_dir)\n",
    "\n",
    "# # Copy images for the new validation set\n",
    "# copy_images(val_data[\"images\"], new_val_dir)\n",
    "\n",
    "# print(\"Images have been copied to the new balanced directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b741bb",
   "metadata": {},
   "source": [
    "### This Cell Splits Checks the distribution (histogram) for train/val annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON files\n",
    "with open(\"/mnt/datassd0/coco-2017/annotations/instances_train2017_balanced.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"/mnt/datassd0/coco-2017/annotations/instances_val2017_balanced.json\", \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# Create a mapping from category ID to category name (assumes both files share same categories)\n",
    "category_mapping = {cat[\"id\"]: cat[\"name\"] for cat in train_data[\"categories\"]}\n",
    "\n",
    "# Initialize dictionaries to hold counts per category for train and validation datasets\n",
    "train_counts = {cat_id: 0 for cat_id in category_mapping}\n",
    "val_counts   = {cat_id: 0 for cat_id in category_mapping}\n",
    "\n",
    "# Count annotations per category for the training set\n",
    "for ann in train_data[\"annotations\"]:\n",
    "    cid = ann[\"category_id\"]\n",
    "    train_counts[cid] += 1\n",
    "\n",
    "# Count annotations per category for the validation set\n",
    "for ann in val_data[\"annotations\"]:\n",
    "    cid = ann[\"category_id\"]\n",
    "    val_counts[cid] += 1\n",
    "\n",
    "# Get sorted lists of category IDs, names, and counts for consistent plotting\n",
    "sorted_ids   = sorted(category_mapping.keys())\n",
    "sorted_names = [category_mapping[cid] for cid in sorted_ids]\n",
    "train_values = [train_counts.get(cid, 0) for cid in sorted_ids]\n",
    "val_values   = [val_counts.get(cid, 0) for cid in sorted_ids]\n",
    "\n",
    "# Set up the bar plot parameters\n",
    "bar_width = 0.35\n",
    "indices   = np.arange(len(sorted_ids))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot bars for the training set\n",
    "plt.bar(indices, train_values, bar_width, label=\"Train\", alpha=0.7, color=\"blue\")\n",
    "\n",
    "# Plot bars for the validation set, shifted by the bar width\n",
    "plt.bar(indices + bar_width, val_values, bar_width, label=\"Validation\", alpha=0.7, color=\"orange\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Annotations\")\n",
    "plt.title(\"Class Distribution in Balanced Train vs. Validation Datasets\")\n",
    "plt.xticks(indices + bar_width / 2, sorted_names, rotation=90)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9eab5b",
   "metadata": {},
   "source": [
    "# 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDataset(data.Dataset):\n",
    "    def __init__(self, image_dir, anno_path, labels_path=None, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.labels_path = labels_path\n",
    "\n",
    "        # Load COCO JSON\n",
    "        with open(anno_path, \"r\") as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        self.images = {img[\"id\"]: img[\"file_name\"] for img in coco_data[\"images\"]}\n",
    "        self.annotations = coco_data[\"annotations\"]\n",
    "\n",
    "        # Store category names along with ID mappings\n",
    "        self.category_map = {c[\"id\"]: i for i, c in enumerate(coco_data[\"categories\"], start=1)}\n",
    "        self.category_id_to_name = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}  \n",
    "        self.num_classes = len(self.category_map)\n",
    "\n",
    "        # Organize annotations by image_id\n",
    "        self.image_to_annotations = {img_id: [] for img_id in self.images}\n",
    "        for ann in self.annotations:\n",
    "            self.image_to_annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "        # Store image IDs as dataset index\n",
    "        self.image_ids = list(self.images.keys())\n",
    "\n",
    "        # Load or generate labels\n",
    "        if self.labels_path and os.path.exists(self.labels_path):\n",
    "            print(f\"Loading precomputed labels from {self.labels_path}...\")\n",
    "            self.labels = np.load(self.labels_path)\n",
    "        else:\n",
    "            print(\"No precomputed label file found. Generating labels...\")\n",
    "            self.labels = self.generate_labels()\n",
    "            if self.labels_path:\n",
    "                os.makedirs(os.path.dirname(self.labels_path), exist_ok=True)\n",
    "                self.save_labels(self.labels_path)\n",
    "\n",
    "    def generate_labels(self):\n",
    "        # Generate one-hot encoded labels for each image and return as NumPy array.\n",
    "        labels = np.zeros((len(self.image_ids), self.num_classes))\n",
    "        for i, img_id in enumerate(self.image_ids):\n",
    "            for ann in self.image_to_annotations[img_id]:\n",
    "                category_id = ann[\"category_id\"]\n",
    "                labels[i][self.category_map[category_id] - 1] = 1  # Convert to one-hot\n",
    "        return labels\n",
    "\n",
    "    def save_labels(self, labels_path):\n",
    "        np.save(labels_path, self.labels)\n",
    "        print(f\"Labels saved to {labels_path}\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Loads an image and its one-hot encoded label.\n",
    "        img_id = self.image_ids[index]\n",
    "        img_path = os.path.join(self.image_dir, self.images[img_id])\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load label from precomputed `.npy`\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f95328",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = \"/mnt/datassd0/coco-2017/images/all_images\"\n",
    "train_annotations = \"/mnt/datassd0/coco-2017/annotations/instances_train2017_balanced.json\"\n",
    "train_labels_npy = \"/mnt/datassd0/coco-2017/annotations/train_labels.npy\"  # Path to save labels\n",
    "\n",
    "val_images = \"/mnt/datassd0/coco-2017/images/all_images\"\n",
    "val_annotations = \"/mnt/datassd0/coco-2017/annotations/instances_val2017_balanced.json\"\n",
    "val_labels_npy = \"/mnt/datassd0/coco-2017/annotations/val_labels.npy\"  # Path to save labels\n",
    "\n",
    "train_dataset = CustomCocoDataset(train_images, train_annotations, labels_path=train_labels_npy)\n",
    "val_dataset = CustomCocoDataset(val_images, val_annotations, labels_path=val_labels_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the category ID assigned to \"unlabeled\"\n",
    "unlabeled_id = None\n",
    "for cat_id, cat_name in train_dataset.category_id_to_name.items():\n",
    "    if cat_name == \"unlabeled\":\n",
    "        unlabeled_id = cat_id\n",
    "        break\n",
    "\n",
    "if unlabeled_id is None:\n",
    "    raise ValueError(\"Error: 'unlabeled' category not found in dataset categories.\")\n",
    "\n",
    "# Count total number of images\n",
    "total_images = len(train_dataset) + len(val_dataset)\n",
    "\n",
    "# Count the number of images with no annotations or only \"unlabeled\" category\n",
    "unlabeled_count = 0\n",
    "\n",
    "# Count unlabeled images in the training dataset\n",
    "for img_id in train_dataset.image_ids:\n",
    "    annotations = train_dataset.image_to_annotations[img_id]\n",
    "    \n",
    "    if not annotations or all(ann[\"category_id\"] == unlabeled_id for ann in annotations):\n",
    "        unlabeled_count += 1\n",
    "\n",
    "# Count unlabeled images in the validation dataset\n",
    "for img_id in val_dataset.image_ids:\n",
    "    annotations = val_dataset.image_to_annotations[img_id]\n",
    "    \n",
    "    if not annotations or all(ann[\"category_id\"] == unlabeled_id for ann in annotations):\n",
    "        unlabeled_count += 1\n",
    "\n",
    "# Compute and print the percentage of unlabeled images\n",
    "unlabeled_percentage = (unlabeled_count / total_images) * 100\n",
    "\n",
    "print(f\"Total images: {total_images}\")\n",
    "print(f\"Unlabeled images: {unlabeled_count}\")\n",
    "print(f\"Unlabeled Percentage: {unlabeled_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60468e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image and label from dataset\n",
    "image, label = train_dataset[1]  \n",
    "\n",
    "# Convert label tensor to numpy array\n",
    "label_array = label.numpy()\n",
    "print(label_array)\n",
    "\n",
    "# Get names of active labels (where value is 1)\n",
    "active_labels = [train_dataset.category_id_to_name[i + 1] for i in range(len(label_array)) if label_array[i] == 1]\n",
    "\n",
    "# Convert image to PIL if it's a tensor\n",
    "if isinstance(image, torch.Tensor):\n",
    "    image = transforms.ToPILImage()(image)\n",
    "\n",
    "# Display the image with label names\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Labels: {', '.join(active_labels)}\")  # Display category names instead of numbers\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quinn-torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
