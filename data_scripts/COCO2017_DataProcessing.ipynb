{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5f4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pycocotools.coco import COCO\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6538113",
   "metadata": {},
   "source": [
    "# 1. Filter Classes and Re-label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ca13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_IDs(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    categories = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "    print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3951b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_IDs(\"/mnt/datassd0/coco-2017/annotations/instances_train2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d19c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_IDs(\"/mnt/datassd0/coco-2017/annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# KEEP IMAGES WITH 0 ANNOTATIONS AND ASSIGN \"UNLABELED\" CATEGORY #################\n",
    "def filter_and_relabel_coco(input_json, output_json, dropped_classes):\n",
    "    \n",
    "    # Load the original COCO dataset\n",
    "    with open(input_json, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Get a mapping of original category IDs to names\n",
    "    original_categories = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "\n",
    "    # Select only categories **not in dropped_classes** and relabel them from 1 to N\n",
    "    selected_categories = [c for c in coco_data[\"categories\"] if c[\"name\"] not in dropped_classes]\n",
    "    selected_cat_ids = {c[\"id\"]: i + 1 for i, c in enumerate(selected_categories)}  # Start from 1 instead of 0\n",
    "\n",
    "    # Add an \"unlabeled\" category at the end\n",
    "    unlabeled_id = len(selected_categories) + 1\n",
    "    new_categories = [{\"id\": new_id, \"name\": c[\"name\"]} for new_id, c in enumerate(selected_categories, start=1)]\n",
    "    new_categories.append({\"id\": unlabeled_id, \"name\": \"unlabeled\"})  # Add \"unlabeled\" category\n",
    "\n",
    "    new_annotations = []\n",
    "    image_id_to_annotations = {}\n",
    "\n",
    "    # Process annotations and keep only non-dropped categories\n",
    "    for ann in coco_data[\"annotations\"]:\n",
    "        if ann[\"category_id\"] in selected_cat_ids:\n",
    "            ann[\"category_id\"] = selected_cat_ids[ann[\"category_id\"]]  # Re-map category ID\n",
    "            new_annotations.append(ann)\n",
    "            image_id_to_annotations.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "    # Get all image IDs that exist in the dataset\n",
    "    all_image_ids = {img[\"id\"] for img in coco_data[\"images\"]}\n",
    "\n",
    "    # Identify images that have no remaining annotations\n",
    "    images_without_annotations = all_image_ids - set(image_id_to_annotations.keys())\n",
    "\n",
    "    # Assign a dummy \"unlabeled\" annotation to images that lost all annotations\n",
    "    for img_id in images_without_annotations:\n",
    "        new_annotations.append({\n",
    "            \"id\": len(new_annotations) + 1,  # Unique annotation ID\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": unlabeled_id,  # Assign to \"unlabeled\"\n",
    "            \"bbox\": [0, 0, 1, 1],  # Placeholder bounding box\n",
    "            \"area\": 1,\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "\n",
    "    # Keep all images, even if they had 0 annotations\n",
    "    new_images = coco_data[\"images\"]\n",
    "\n",
    "    new_coco_data = {\n",
    "        \"info\": coco_data.get(\"info\", {}),\n",
    "        \"licenses\": coco_data.get(\"licenses\", []),\n",
    "        \"images\": new_images,\n",
    "        \"annotations\": new_annotations,\n",
    "        \"categories\": new_categories,\n",
    "    }\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(new_coco_data, f, indent=None)\n",
    "\n",
    "    print(f\"Filtered COCO dataset saved as {output_json}\")\n",
    "\n",
    "# Define dropped classes\n",
    "dropped_classes = [\n",
    "    \"person\", # Highest class label\n",
    "    \"wine glass\", \"cup\", \"bicycle\", \"potted plant\", \"bowl\", # Repetitive classes\n",
    "    \"snowboard\", \"surfboard\", \"baseball glove\", \"baseball bat\", # Sports category (highly associated with person)\n",
    "    \"tennis racket\", \"kite\", \"frisbee\", \"skis\", \"sports ball\", \"skateboard\", # Sports category (highly associated with person)\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", # Accessories category (highly associated with person)\n",
    "    \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\" # Vehicles category (highly associated with person)\n",
    "    \"dining table\", # Single class highly associated with person\n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", # Outdoor objects\n",
    "    \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", # Outdoor wild animals\n",
    "]\n",
    "\n",
    "\n",
    "# Apply the filtering function\n",
    "filter_and_relabel_coco(\"/mnt/datassd0/coco-2017/annotations/instances_train2017.json\", \"/mnt/datassd0/coco-2017/annotations/instances_train2017_filtered.json\", dropped_classes)\n",
    "filter_and_relabel_coco(\"/mnt/datassd0/coco-2017/annotations/instances_val2017.json\", \"/mnt/datassd0/coco-2017/annotations/instances_val2017_filtered.json\", dropped_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_IDs(\"/mnt/datassd0/coco-2017/annotations/instances_train2017_filtered.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d920fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_IDs(\"/mnt/datassd0/coco-2017/annotations/instances_val2017_filtered.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e681f88",
   "metadata": {},
   "source": [
    "# 2. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_split_coco_with_images(\n",
    "    train_json, val_json,\n",
    "    output_dir,\n",
    "    original_train_dir, original_val_dir,\n",
    "    split_mode=\"train_val\",                # \"train_val\" or \"train_val_test\"\n",
    "    split_ratios=(0.8, 0.2),               # (train, val) or (train, val, test)\n",
    "    random_seed=42\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load original datasets\n",
    "    with open(train_json, \"r\") as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(val_json, \"r\") as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    # Combine images and annotations\n",
    "    all_images = train_data[\"images\"] + val_data[\"images\"]\n",
    "    all_annotations = train_data[\"annotations\"] + val_data[\"annotations\"]\n",
    "    categories = train_data[\"categories\"]\n",
    "\n",
    "    # Map image_id â†’ annotations\n",
    "    image_id_to_annotations = {img[\"id\"]: [] for img in all_images}\n",
    "    for ann in all_annotations:\n",
    "        image_id_to_annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    all_image_ids = list(image_id_to_annotations.keys())\n",
    "    image_labels = [\n",
    "        (anns[0][\"category_id\"] if anns else -1)\n",
    "        for anns in [image_id_to_annotations[iid] for iid in all_image_ids]\n",
    "    ]\n",
    "\n",
    "    def split_data(ids_set):\n",
    "        imgs = [img for img in all_images if img[\"id\"] in ids_set]\n",
    "        anns = [ann for ann in all_annotations if ann[\"image_id\"] in ids_set]\n",
    "        return imgs, anns\n",
    "\n",
    "    def save_json(images, annotations, name):\n",
    "        data = {\n",
    "            \"info\": train_data.get(\"info\", {}),\n",
    "            \"licenses\": train_data.get(\"licenses\", []),\n",
    "            \"images\": images,\n",
    "            \"annotations\": annotations,\n",
    "            \"categories\": categories,\n",
    "        }\n",
    "        out_path = os.path.join(output_dir, f\"instances_{name}.json\")\n",
    "        with open(out_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=None)\n",
    "        return out_path\n",
    "\n",
    "    # def copy_images(image_list, destination_dir):\n",
    "    #     os.makedirs(destination_dir, exist_ok=True)\n",
    "    #     for img in image_list:\n",
    "    #         file_name = img[\"file_name\"]\n",
    "    #         src_path = os.path.join(original_train_dir, file_name)\n",
    "    #         if not os.path.exists(src_path):\n",
    "    #             src_path = os.path.join(original_val_dir, file_name)\n",
    "    #         if not os.path.exists(src_path):\n",
    "    #             print(f\"Warning: {file_name} not found.\")\n",
    "    #             continue\n",
    "    #         dst_path = os.path.join(destination_dir, file_name)\n",
    "    #         shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    # === SPLITTING LOGIC ===\n",
    "    if split_mode == \"train_val\":\n",
    "        assert len(split_ratios) == 2, \"split_ratios must be (train, val) for train_val mode\"\n",
    "        train_ratio = split_ratios[0]\n",
    "\n",
    "        train_ids, val_ids = train_test_split(\n",
    "            all_image_ids,\n",
    "            train_size=train_ratio,\n",
    "            stratify=image_labels,\n",
    "            random_state=random_seed\n",
    "        )\n",
    "\n",
    "        train_imgs, train_anns = split_data(set(train_ids))\n",
    "        val_imgs, val_anns = split_data(set(val_ids))\n",
    "\n",
    "        save_json(train_imgs, train_anns, \"train\")\n",
    "        save_json(val_imgs, val_anns, \"test\")\n",
    "\n",
    "        # If you want to copy the images to the output directory, uncomment the following lines\n",
    "        # copy_images(train_imgs, os.path.join(output_dir, \"train\"))\n",
    "        # copy_images(val_imgs, os.path.join(output_dir, \"test\"))\n",
    "\n",
    "        print(f\"[Train/Val] Train: {len(train_imgs)}, Test: {len(val_imgs)}\")\n",
    "\n",
    "    elif split_mode == \"train_val_test\":\n",
    "        assert len(split_ratios) == 3, \"split_ratios must be (train, val, test) for train_val_test mode\"\n",
    "        train_ratio, val_ratio, test_ratio = split_ratios\n",
    "        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
    "\n",
    "        # Step 1: Train vs Temp (Val + Test)\n",
    "        train_ids, temp_ids, train_labels, temp_labels = train_test_split(\n",
    "            all_image_ids,\n",
    "            image_labels,\n",
    "            train_size=train_ratio,\n",
    "            stratify=image_labels,\n",
    "            random_state=random_seed\n",
    "        )\n",
    "\n",
    "        # Step 2: Val vs Test from Temp\n",
    "        val_relative = val_ratio / (val_ratio + test_ratio)\n",
    "        val_ids, test_ids = train_test_split(\n",
    "            temp_ids,\n",
    "            train_size=val_relative,\n",
    "            stratify=temp_labels,\n",
    "            random_state=random_seed\n",
    "        )\n",
    "\n",
    "        train_imgs, train_anns = split_data(set(train_ids))\n",
    "        val_imgs, val_anns = split_data(set(val_ids))\n",
    "        test_imgs, test_anns = split_data(set(test_ids))\n",
    "\n",
    "        save_json(train_imgs, train_anns, \"train\")\n",
    "        save_json(val_imgs, val_anns, \"val\")\n",
    "        save_json(test_imgs, test_anns, \"test\")\n",
    "\n",
    "        # If you want to copy the images to the output directory, uncomment the following lines\n",
    "        # copy_images(train_imgs, os.path.join(output_dir, \"train\"))\n",
    "        # copy_images(val_imgs, os.path.join(output_dir, \"val\"))\n",
    "        # copy_images(test_imgs, os.path.join(output_dir, \"test\"))\n",
    "\n",
    "        print(f\"[Train/Val/Test] Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split_mode. Use 'train_val' or 'train_val_test'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_and_split_coco_with_images(\n",
    "    train_json=\"/mnt/datassd0/coco-2017/annotations/instances_train2017_filtered.json\",\n",
    "    val_json=\"/mnt/datassd0/coco-2017/annotations/instances_val2017_filtered.json\",\n",
    "    output_dir=\"/mnt/datassd0/coco-2017/output_balanced_80_20\",\n",
    "    original_train_dir=\"/mnt/datassd0/coco-2017/images/train2017\",\n",
    "    original_val_dir=\"/mnt/datassd0/coco-2017/images/val2017\",\n",
    "    split_mode=\"train_val\",\n",
    "    split_ratios=(0.8, 0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_and_split_coco_with_images(\n",
    "    train_json=\"/mnt/datassd0/coco-2017/annotations/instances_train2017_filtered.json\",\n",
    "    val_json=\"/mnt/datassd0/coco-2017/annotations/instances_val2017_filtered.json\",\n",
    "    output_dir=\"/mnt/datassd0/coco-2017/output_balanced_70_15_15\",\n",
    "    original_train_dir=\"/mnt/datassd0/coco-2017/images/train2017\",\n",
    "    original_val_dir=\"/mnt/datassd0/coco-2017/images/val2017\",\n",
    "    split_mode=\"train_val_test\",\n",
    "    split_ratios=(0.7, 0.15, 0.15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b741bb",
   "metadata": {},
   "source": [
    "### This Cell Splits Checks the distribution (histogram) for train/val annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON files\n",
    "with open(\"/mnt/datassd0/coco-2017/output_balanced_70_15_15/instances_train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"/mnt/datassd0/coco-2017/output_balanced_70_15_15/instances_val.json\", \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open(\"/mnt/datassd0/coco-2017/output_balanced_70_15_15/instances_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Create a mapping from category ID to name (assumes same categories in all files)\n",
    "category_mapping = {cat[\"id\"]: cat[\"name\"] for cat in train_data[\"categories\"]}\n",
    "\n",
    "# Initialize counts for all three sets\n",
    "train_counts = {cid: 0 for cid in category_mapping}\n",
    "val_counts   = {cid: 0 for cid in category_mapping}\n",
    "test_counts  = {cid: 0 for cid in category_mapping}\n",
    "\n",
    "# Count annotations\n",
    "for ann in train_data[\"annotations\"]:\n",
    "    train_counts[ann[\"category_id\"]] += 1\n",
    "\n",
    "for ann in val_data[\"annotations\"]:\n",
    "    val_counts[ann[\"category_id\"]] += 1\n",
    "\n",
    "for ann in test_data[\"annotations\"]:\n",
    "    test_counts[ann[\"category_id\"]] += 1\n",
    "\n",
    "# Prepare for plotting\n",
    "sorted_ids   = sorted(category_mapping.keys())\n",
    "sorted_names = [category_mapping[cid] for cid in sorted_ids]\n",
    "train_values = [train_counts.get(cid, 0) for cid in sorted_ids]\n",
    "val_values   = [val_counts.get(cid, 0) for cid in sorted_ids]\n",
    "test_values  = [test_counts.get(cid, 0) for cid in sorted_ids]\n",
    "\n",
    "bar_width = 0.25\n",
    "indices = np.arange(len(sorted_ids))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot bars\n",
    "plt.bar(indices, train_values, bar_width, label=\"Train\", alpha=0.7, color=\"blue\")\n",
    "plt.bar(indices + bar_width, val_values, bar_width, label=\"Validation\", alpha=0.7, color=\"orange\")\n",
    "plt.bar(indices + 2 * bar_width, test_values, bar_width, label=\"Test\", alpha=0.7, color=\"green\")\n",
    "\n",
    "# Labels and aesthetics\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Annotations\")\n",
    "plt.title(\"Class Distribution in Train / Val / Test Datasets\")\n",
    "plt.xticks(indices + bar_width, sorted_names, rotation=90)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9eab5b",
   "metadata": {},
   "source": [
    "# 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDataset(data.Dataset):\n",
    "    def __init__(self, image_dir, anno_path, labels_path=None, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.labels_path = labels_path\n",
    "\n",
    "        # Load COCO JSON\n",
    "        with open(anno_path, \"r\") as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        self.images = {img[\"id\"]: img[\"file_name\"] for img in coco_data[\"images\"]}\n",
    "        self.annotations = coco_data[\"annotations\"]\n",
    "\n",
    "        # Store category names along with ID mappings\n",
    "        self.category_map = {c[\"id\"]: i for i, c in enumerate(coco_data[\"categories\"], start=1)}\n",
    "        self.category_id_to_name = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}  \n",
    "        self.num_classes = len(self.category_map)\n",
    "\n",
    "        # Organize annotations by image_id\n",
    "        self.image_to_annotations = {img_id: [] for img_id in self.images}\n",
    "        for ann in self.annotations:\n",
    "            self.image_to_annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "        # Store image IDs as dataset index\n",
    "        self.image_ids = list(self.images.keys())\n",
    "\n",
    "        # Load or generate labels\n",
    "        if self.labels_path and os.path.exists(self.labels_path):\n",
    "            print(f\"Loading precomputed labels from {self.labels_path}...\")\n",
    "            self.labels = np.load(self.labels_path)\n",
    "        else:\n",
    "            print(\"No precomputed label file found. Generating labels...\")\n",
    "            self.labels = self.generate_labels()\n",
    "            if self.labels_path:\n",
    "                os.makedirs(os.path.dirname(self.labels_path), exist_ok=True)\n",
    "                self.save_labels(self.labels_path)\n",
    "\n",
    "    def generate_labels(self):\n",
    "        # Generate binary encoded labels for each image and return as NumPy array.\n",
    "        labels = np.zeros((len(self.image_ids), self.num_classes))\n",
    "        for i, img_id in enumerate(self.image_ids):\n",
    "            for ann in self.image_to_annotations[img_id]:\n",
    "                category_id = ann[\"category_id\"]\n",
    "                labels[i][self.category_map[category_id] - 1] = 1  # Convert to binary\n",
    "        return labels\n",
    "\n",
    "    def save_labels(self, labels_path):\n",
    "        np.save(labels_path, self.labels)\n",
    "        print(f\"Labels saved to {labels_path}\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Loads an image and its binary encoded label.\n",
    "        img_id = self.image_ids[index]\n",
    "        img_path = os.path.join(self.image_dir, self.images[img_id])\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load label from precomputed `.npy`\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f95328",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = \"/mnt/datassd0/coco-2017/images/all_images\"\n",
    "train_annotations = \"/mnt/datassd0/coco-2017/output_balanced_70_15_15/instances_train.json\"\n",
    "train_labels_npy = \"/mnt/datassd0/coco-2017/annotations/train_labels.npy\"  # Path to save labels\n",
    "\n",
    "val_images = \"/mnt/datassd0/coco-2017/images/all_images\"\n",
    "val_annotations = \"/mnt/datassd0/coco-2017/output_balanced_70_15_15/instances_val.json\"\n",
    "val_labels_npy = \"/mnt/datassd0/coco-2017/annotations/val_labels.npy\"  # Path to save labels\n",
    "\n",
    "train_dataset = CustomCocoDataset(train_images, train_annotations, labels_path=train_labels_npy)\n",
    "val_dataset = CustomCocoDataset(val_images, val_annotations, labels_path=val_labels_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the category ID assigned to \"unlabeled\"\n",
    "unlabeled_id = None\n",
    "for cat_id, cat_name in train_dataset.category_id_to_name.items():\n",
    "    if cat_name == \"unlabeled\":\n",
    "        unlabeled_id = cat_id\n",
    "        break\n",
    "\n",
    "if unlabeled_id is None:\n",
    "    raise ValueError(\"Error: 'unlabeled' category not found in dataset categories.\")\n",
    "\n",
    "# Count total number of images\n",
    "total_images = len(train_dataset) + len(val_dataset)\n",
    "\n",
    "# Count the number of images with no annotations or only \"unlabeled\" category\n",
    "unlabeled_count = 0\n",
    "\n",
    "# Count unlabeled images in the training dataset\n",
    "for img_id in train_dataset.image_ids:\n",
    "    annotations = train_dataset.image_to_annotations[img_id]\n",
    "    \n",
    "    if not annotations or all(ann[\"category_id\"] == unlabeled_id for ann in annotations):\n",
    "        unlabeled_count += 1\n",
    "\n",
    "# Count unlabeled images in the validation dataset\n",
    "for img_id in val_dataset.image_ids:\n",
    "    annotations = val_dataset.image_to_annotations[img_id]\n",
    "    \n",
    "    if not annotations or all(ann[\"category_id\"] == unlabeled_id for ann in annotations):\n",
    "        unlabeled_count += 1\n",
    "\n",
    "# Compute and print the percentage of unlabeled images\n",
    "unlabeled_percentage = (unlabeled_count / total_images) * 100\n",
    "\n",
    "print(f\"Total images: {total_images}\")\n",
    "print(f\"Unlabeled images: {unlabeled_count}\")\n",
    "print(f\"Unlabeled Percentage: {unlabeled_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60468e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image and label from dataset\n",
    "image, label = train_dataset[1]  \n",
    "\n",
    "# Convert label tensor to numpy array\n",
    "label_array = label.numpy()\n",
    "print(label_array)\n",
    "\n",
    "# Get names of active labels (where value is 1)\n",
    "active_labels = [train_dataset.category_id_to_name[i + 1] for i in range(len(label_array)) if label_array[i] == 1]\n",
    "\n",
    "# Convert image to PIL if it's a tensor\n",
    "if isinstance(image, torch.Tensor):\n",
    "    image = transforms.ToPILImage()(image)\n",
    "\n",
    "# Display the image with label names\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Labels: {', '.join(active_labels)}\")  # Display category names instead of numbers\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quinn-torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
