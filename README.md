# Multi-Label Classification in the Presence of Negative Data: Improving Any or No Class Presence Awareness

## Overview

This project introduces a novel auxiliary loss function designed to enhance multi-label classification (MLC) tasks, particularly in scenarios with a large number of negative samples (data points not belonging to any defined class). The auxiliary loss acts as a complementary term to standard MLC loss functions.

## Features

* Implementation of a novel auxiliary loss term for MLC.
* Focus on improving performance in the presence of many negative samples.
* Modular structure using PyTorch Lightning.

## Project Structure

```
.
├── data_scripts/       # Scripts for data downloading and preprocessing
├── gmean_mlc/          # Core PyTorch code: datasets, dataloaders, models, lightning modules, metrics
├── scripts/            # Main executable scripts for training, inference, etc.
```

## Setup

1. **Clone the repository:**

   ```bash
   git clone <your-repository-url>
   cd <your-repository-name>
   ```
2. **Install dependencies:**
   It's recommended to use a virtual environment (like `venv` or `conda`).

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   pip install -r requirements.txt
   ```

## Data Preparation

The scripts required for downloading and preparing the datasets used in this project are located in the `data_scripts/` directory. Please refer to the specific scripts within that directory for instructions on how to prepare your data.

The PyTorch `Dataset` and `LightningDataModule` implementations can be found in the `gmean_mlc/` directory.

## Usage

The primary scripts for running experiments are located in the `scripts/` directory.

* **Training (`scripts/lightning_trainer.py`):** This script handles model training using PyTorch Lightning.

  *Example command:*

  ```bash
  python scripts/lightning_trainer.py \
      --dataset coco \
      --wandb_project gmean-mlc-coco \
      --ann_root /path/to/coco-2017/annotations/ \
      --data_root /path/to/coco-2017/images/ \
      --workers 8 \
      --log_save_dir /path/to/training_logs/ \
      --precision 16-mixed \
      --matmul_precision high \
      --model maxvit_s \
      --max_epochs 90 \
      --optimizer_type adamW \
      --learning_rate 1e-3 \
      --min_lr 1e-5 \
      --warmup_steps 5 \
      --warmup_start_factor 0.01 \
      --weight_decay 0.05 \
      --base_loss focal \
      --class_balancing_beta 0.9999 \
      --meta_loss_weight 1 \
      --meta_loss_beta 0.02 \
      --gpus 0 1 \
      --log_version <your_log_version> # e.g., 256
  ```
* **Inference (`scripts/inference.py`):** Loads trained model checkpoints and performs inference on a specified data split (e.g., validation or test set). It saves the raw model outputs (scores/predictions) to CSV files.

  *Example command:*

  ```bash
  python scripts/inference.py \
      --dataset coco \
      --ann_root /path/to/coco-2017/annotations/ \
      --data_root /path/to/coco-2017/images/ \
      --batch_size 256 \
      --log_dir /path/to/coco-2017/training_logs/ \
      --versions <log_version_1> <log_version_2> ... # e.g., 100 101 102 \
      --results_output results/results-coco \
      --split Val \
      --device_id 0
  ```

  *Note: Replace `<log_version_1>` etc. with the actual log versions generated during training.*
* **Result Calculation (`scripts/calculate_results.py`):** Takes the CSV output files generated by `scripts/inference.py` and the corresponding annotations to calculate evaluation metrics.

  *Example command:*

  ```bash
  python scripts/calculate_results.py \
      --dataset coco \
      --ann_root /path/to/coco-2017/annotations/ \
      --data_root /path/to/coco-2017/images/all_images/ \
      --split Val \
      --score_dir results/results-coco/ \
      --versions <log_version_1> <log_version_2> ... # e.g., 100 101 102
  ```

  *Note: The `--version` arguments should correspond to the log versions for which inference was run.*

*Please consult the individual scripts for a full list of arguments and their descriptions (e.g., using `python scripts/lightning_trainer.py --help`).*

## Results

The `results/` directory is used to store the outputs generated by `scripts/inference.py` and `scripts/calculate_results.py`. This directory is included in `.gitignore` and is not tracked by version control.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
